{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "id": "_ngDCL-Ucc8F",
    "outputId": "bdf787c6-0c54-413a-9a6f-e3c5cc9f02bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Enter the option: For the hotel review data : hotel : For the movie review data : movie\n",
      "movie\n",
      "Enter the input phrase for movie\n",
      "poor story. not fascinating \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fascinating', 'story', 'poor']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:235: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:238: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie Review\n",
      "fascinating ['crawford', 'aoki', 'mayo', 'bush', 'fjkdfa', 'travolta', 'miljan', 'baragrey', 'gish', 'sander']\n",
      "\n",
      "\n",
      "story ['story', 'jessica', 'begin', 'cameo', 'nevada', 'holland', 'blockbuster', 'lust', 'borzage', 'full']\n",
      "\n",
      "\n",
      "poor ['poor', 'merk', 'zero', 'anar', 'culp', 'text', 'lizzzzzzzz', 'anymore', 'connor', 'html']\n",
      "\n",
      "\n",
      "5810.046062707901\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\n\\nimport sys\\nL=list(sys.argv[1:])\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting Similar Words for each key words in the phrase\n",
    "import time\n",
    "start = time.time()\n",
    "import time\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec\n",
    "from scipy import spatial\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import random\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')\n",
    "sto = set(stopwords.words(\"english\"))\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stopwords=[]\n",
    "for k in sto:\n",
    "    stopwords.append(k)\n",
    "\n",
    "flags = (re.UNICODE if sys.version < '3' and type(text) is unicode else 0)\n",
    "\n",
    "class data_preprocessing:\n",
    "    @classmethod\n",
    "    def hotel_data(cls):\n",
    "                    hotel=pd.read_csv(\"Hotel_Reviews.csv\")\n",
    "                    revid=0\n",
    "                    Words={}\n",
    "\n",
    "                    cn=0\n",
    "                    cn1=0\n",
    "                    for k in hotel['Negative_Review']:\n",
    "                        keep=[]\n",
    "                        if cn<10000:\n",
    "                                #print(k)\n",
    "                                for word in re.findall(r\"\\w[\\w']*[^0-9]\", k, flags=flags):\n",
    "                                    if word.isdigit() or len(word)==1:\n",
    "                                        continue\n",
    "                                    word_lower = word.lower()\n",
    "                                    if word_lower in stopwords:\n",
    "                                            continue\n",
    "                                    word1 = wordnet_lemmatizer.lemmatize(word_lower, pos = \"n\")\n",
    "                                    word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\n",
    "                                    word3 = wordnet_lemmatizer.lemmatize(word2, pos = (\"a\"))\n",
    "                                    if not any(c.isdigit() for c in  word3 ) and \"'\" not in word3:\n",
    "                                        if  word3 .isdigit():\n",
    "                                            continue\n",
    "                                        else:\n",
    "                                             if len(word3)>=3:\n",
    "                                                    keep.append(word3)\n",
    "                                if len(keep)>=10:\n",
    "                                        Words[revid]=keep\n",
    "                                        revid=revid+1\n",
    "                                cn=cn+1\n",
    "                    for k in hotel['Positive_Review']:\n",
    "                        keep=[]\n",
    "                        if cn1<10000:\n",
    "                                #print(k)\n",
    "                                for word in re.findall(r\"\\w[\\w']*[^0-9]\", k, flags=flags):\n",
    "                                    if word.isdigit() or len(word)==1:\n",
    "                                        continue\n",
    "                                    word_lower = word.lower()\n",
    "                                    if word_lower in stopwords:\n",
    "                                            continue\n",
    "                                    word1 = wordnet_lemmatizer.lemmatize(word_lower, pos = \"n\")\n",
    "                                    word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\n",
    "                                    word3 = wordnet_lemmatizer.lemmatize(word2, pos = (\"a\"))\n",
    "                                    if not any(c.isdigit() for c in  word3 ) and \"'\" not in word3:\n",
    "                                        if  word3 .isdigit():\n",
    "                                            continue\n",
    "                                        else:\n",
    "                                             if len(word3)>=3:\n",
    "                                                    keep.append(word3)\n",
    "                                if len(keep)>=10:\n",
    "                                        Words[revid]=keep\n",
    "                                        revid=revid+1\n",
    "\n",
    "                                cn1=cn1+1\n",
    "                                \n",
    "                    return Words\n",
    "    @classmethod\n",
    "    def movie_data(cls):\n",
    "                    f22=pd.read_csv(\"IMDB Dataset.csv\")\n",
    "                    Words1={}\n",
    "                    rid=0\n",
    "                    cn2=0\n",
    "                    for k in f22['review']:\n",
    "                        keep=[]\n",
    "                        if cn2<50000:\n",
    "                                #print(k)\n",
    "                                for word in re.findall(r\"\\w[\\w']*[^0-9]\", k, flags=flags):\n",
    "                                    if word.isdigit() or len(word)==1:\n",
    "                                        continue\n",
    "                                    word_lower = word.lower()\n",
    "                                    if word_lower in stopwords:\n",
    "                                            continue\n",
    "                                    word1 = wordnet_lemmatizer.lemmatize(word_lower, pos = \"n\")\n",
    "                                    word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\n",
    "                                    word3 = wordnet_lemmatizer.lemmatize(word2, pos = (\"a\"))\n",
    "                                    if not any(c.isdigit() for c in   word3 ) and \"'\" not in  word3:\n",
    "                                        if   word3.isdigit():\n",
    "                                            continue\n",
    "                                        else:\n",
    "                                             if len(word3)>3:\n",
    "                                                    keep.append(word3)\n",
    "                                if len(keep)>=25 :#and len(keep)<=40:\n",
    "                                        Words1[rid]=keep\n",
    "                                        rid=rid+1\n",
    "\n",
    "                                cn2=cn2+1  \n",
    "                \n",
    "                    return Words1\n",
    "                \n",
    "                \n",
    "    @classmethod\n",
    "    def sent_generation(cls,Words):\n",
    "            sent=[]                         \n",
    "            for k in Words:\n",
    "                gh=[]\n",
    "                jj=str(k)\n",
    "                gh.append(jj)\n",
    "                for v in Words[k]:\n",
    "                    gh.append(v)\n",
    "                sent.append(gh)\n",
    "            return sent\n",
    "    @classmethod\n",
    "    #K-Means \n",
    "    def cluster(cls,Words):\n",
    "                    #cluster generation with k-means\n",
    "                    import sys\n",
    "                    from nltk.cluster import KMeansClusterer\n",
    "                    import nltk\n",
    "                    from sklearn import cluster\n",
    "                    from sklearn import metrics\n",
    "                    import gensim \n",
    "                    import operator\n",
    "                    from gensim.models import Word2Vec\n",
    "\n",
    "                    #Words=hotel_d\n",
    "\n",
    "                    model = Word2Vec(Words, min_count=1)\n",
    "\n",
    "                    X = model[model.wv.vocab]\n",
    "\n",
    "\n",
    "\n",
    "                    NUM_CLUSTERS=10\n",
    "                    kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=nltk.cluster.util.cosine_distance,repeats=25)\n",
    "                    assigned_clusters1 = kclusterer.cluster(X,assign_clusters=True)\n",
    "                    #print (assigned_clusters)\n",
    "                    cluster={}\n",
    "                    words = list(model.wv.vocab)\n",
    "                    for i, word in enumerate(words):\n",
    "                      gh=[] \n",
    "                      gh1=[] \n",
    "                      gh2=[] \n",
    "                      if word.isdigit(): \n",
    "                        cluster[word]=assigned_clusters1[i]\n",
    "                        #print (word + \":\" + str(assigned_clusters[i]))\n",
    "                    cluster_final={}\n",
    "                    for j in range(NUM_CLUSTERS):\n",
    "                        gg=[]\n",
    "                        for tt in cluster:\n",
    "                            if int(cluster[tt])==int(j):\n",
    "                                if tt not in gg:\n",
    "                                    gg.append(tt)\n",
    "                        if len(gg)>0:\n",
    "                                    cluster_final[j]=gg\n",
    "                    cc=0\n",
    "                    final_clu={}\n",
    "                    for t in cluster_final:\n",
    "                        ghh=[]\n",
    "                        for k in cluster_final[t]:\n",
    "                            if int(k) in Words:\n",
    "                                   ghh.append(int(k))\n",
    "                        if len(ghh)>=2:\n",
    "                                final_clu[cc]=ghh\n",
    "                                cc=cc+1\n",
    "                    return final_clu\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Find the similar words from the input data using user input\n",
    "class similar_phrase:\n",
    "    @classmethod\n",
    "    def same_words(cls,sent,m,Words):\n",
    "                        print(\"Enter the input phrase for \"+m)\n",
    "                        input_text1=input()\n",
    "                        text=str(input_text1)\n",
    "                        n_gram_range = (1, 1)\n",
    "                        stop_words = \"english\"\n",
    "                        # Extract candidate words/phrases\n",
    "                        from sklearn.feature_extraction.text import CountVectorizer\n",
    "                        from sklearn.metrics.pairwise import cosine_similarity\n",
    "                        count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([text])\n",
    "                        candidates = count.get_feature_names()\n",
    "                        from sentence_transformers import SentenceTransformer\n",
    "                        model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "                        doc_embedding = model.encode([text])\n",
    "                        candidate_embeddings = model.encode(candidates)\n",
    "                        top_n =20\n",
    "                        distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "                        keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "                        vc=keywords #[input_text1]\n",
    "                        print(vc)\n",
    "                        sent.append(vc)\n",
    "                        model = Word2Vec(sent, min_count=1,iter=10)\n",
    "                        #result = model.most_similar(positive=[input_text1],  topn=20)\n",
    "                        #for t in result:\n",
    "                            #if t[0].isdigit():\n",
    "                                #continue\n",
    "                          #  else:\n",
    "                                 # print(t)\n",
    "                        \n",
    "                        unique_words=[]\n",
    "                        ss=[]\n",
    "                        for t in Words:\n",
    "                            for kk in Words[t]:\n",
    "                                if kk not in ss:\n",
    "                                    ss.append(kk)\n",
    "                        s=set(ss)\n",
    "                        for v in s:\n",
    "                            unique_words.append(v)\n",
    "                        wrdvec={}\n",
    "                        for vb in unique_words:\n",
    "                            wrdvec[vb]=model[vb]\n",
    "                        phrase_in={}\n",
    "                        for k in vc:\n",
    "                            phrase_in[k]=model[k]\n",
    "\n",
    "                        \n",
    "                        \n",
    "                        dp={}\n",
    "                        for k in vc:\n",
    "                                dc={}\n",
    "                                wrd_sim=[]\n",
    "                                for bb in unique_words:\n",
    "                                    vc1=wrdvec[bb]\n",
    "\n",
    "                                    vc2=phrase_in[k]\n",
    "                                    #sm=1 - spatial.distance.cosine(vc1, vc2)\n",
    "                                    sm1=dot(vc1,vc2)/(norm(vc1)*norm(vc2))\n",
    "                                    dc[bb]=sm1\n",
    "                                import operator\n",
    "                                sorted_sim_map1 = sorted(dc.items(), key=operator.itemgetter(1),reverse=True)\n",
    "                                cv=0\n",
    "                                for vc in  sorted_sim_map1:\n",
    "                                  if len(vc[0])>=4:\n",
    "                                      if cv<10:\n",
    "                                        if vc[0] not in wrd_sim:\n",
    "                                          if vc[0].isalnum():\n",
    "                                              wrd_sim.append(vc[0])\n",
    "                                              cv=cv+1\n",
    "                                dp[k]=wrd_sim\n",
    "                                #print(wrd_sim)\n",
    "                        return dp\n",
    "    @classmethod\n",
    "    def same_words_cl(cls,sent,m,Words1,cl):\n",
    "                        print(\"Enter the input phrase for \"+m)\n",
    "                        input_text1=input()\n",
    "                        vc=[input_text1]\n",
    "                        Words={}\n",
    "                        for t in Words1:\n",
    "                            if str(t) in cl or int(t) in cl:\n",
    "                                Words[t]=Words1[t]\n",
    "                        sent=[]                         \n",
    "                        for k in Words:\n",
    "                            gh=[]\n",
    "                            jj=str(k)\n",
    "                            gh.append(jj)\n",
    "                            for v in Words[k]:\n",
    "                                gh.append(v)\n",
    "                            sent.append(gh)\n",
    "                        sent.append(vc)\n",
    "                        model = Word2Vec(sent, min_count=1,iter=100)\n",
    "                        #result = model.most_similar(positive=[input_text1],  topn=20)\n",
    "                        #for t in result:\n",
    "                            #if t[0].isdigit():\n",
    "                                #continue\n",
    "                          #  else:\n",
    "                                 # print(t)\n",
    "                        unique_words=[]\n",
    "                        ss=[]\n",
    "                        for t in Words:\n",
    "                            for kk in Words[t]:\n",
    "                                if kk not in ss:\n",
    "                                    ss.append(kk)\n",
    "                        s=set(ss)\n",
    "                        for v in s:\n",
    "                            unique_words.append(v)\n",
    "                        wrdvec={}\n",
    "                        for vb in unique_words:\n",
    "                            wrdvec[vb]=model[vb]\n",
    "                        phrase_in={}\n",
    "                        phrase_in[input_text1]=model[input_text1]\n",
    "\n",
    "                        wrd_sim=[]\n",
    "                        dc={}\n",
    "                        for bb in unique_words:\n",
    "                            vc1=wrdvec[bb]\n",
    "                            vc2=phrase_in[input_text1]\n",
    "                            #sm=1 - spatial.distance.cosine(vc1, vc2)\n",
    "                            sm1=dot(vc1,vc2)/(norm(vc1)*norm(vc2))\n",
    "                            dc[bb]=sm1\n",
    "                        import operator\n",
    "                        sorted_sim_map1 = sorted(dc.items(), key=operator.itemgetter(1),reverse=True)\n",
    "                        cv=0\n",
    "                        for vc in  sorted_sim_map1:\n",
    "                            if cv<25:\n",
    "                              if vc[0].isalnum():\n",
    "                                wrd_sim.append(vc[0])\n",
    "                                cv=cv+1\n",
    "                        #print(wrd_sim)\n",
    "                        return wrd_sim\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "                        #1 - spatial.distance.cosine(vector1, vector2)\n",
    "                        \n",
    "#mwords=similar_phrase.same_words(movie_sent,\"movie\",movie_d)\n",
    "#print(\"Movie Review\")\n",
    "#print(mwords)\n",
    "print(\"Enter the option: For the hotel review data : hotel : For the movie review data : movie\")\n",
    "LL=input()\n",
    "if LL=='hotel':\n",
    "        hotel_d=data_preprocessing.hotel_data()\n",
    "        hotel_sent=data_preprocessing.sent_generation(hotel_d)\n",
    "\n",
    "        #hotel_cluster=data_preprocessing.cluster(hotel_d)\n",
    "        #movie_cluster=data_preprocessing.cluster(movie_d)\n",
    "        hwords=similar_phrase.same_words(hotel_sent,\"hotel\",hotel_d)\n",
    "        print(\"Hotel Review\")\n",
    "        for bb in hwords:\n",
    "            print(bb,hwords[bb])\n",
    "            print(\"\\n\")\n",
    "        #print(hwords)\n",
    "elif LL=='movie':\n",
    "        movie_d=data_preprocessing.movie_data()\n",
    "        movie_sent=data_preprocessing.sent_generation(movie_d)\n",
    "        mwords=similar_phrase.same_words(movie_sent,\"movie\",movie_d)\n",
    "        print(\"Movie Review\")\n",
    "        for bb in mwords:\n",
    "            print(bb,mwords[bb])\n",
    "            print(\"\\n\")\n",
    "        #print(mwords)\n",
    "#for k in hotel_cluster:\n",
    "    #pass#hwords=similar_phrase.same_words_cl(hotel_sent,\"hotel\",hotel_d,hotel_cluster[k])\n",
    "   # pass#print(\"Hotel Review\"+\"cluster_\"+str(k))\n",
    "    #pass#print(hwords)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "'''\n",
    "\n",
    "import sys\n",
    "L=list(sys.argv[1:])\n",
    "\n",
    "'''\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "slwIRQW-W2kX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8J3xPFbAVbxe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDsXTiSFikMK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7c5dGD15ik3c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U7eO0_v3dBNo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Word2Vec_Test2_update.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
